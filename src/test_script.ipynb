{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of FLIR RGB images:  10319\n",
      "Number of FLIR Thermal images:  10742\n",
      "Number of LLVIP RGB images:  12025\n",
      "Number of LLVIP Thermal images:  12025\n",
      "Loading annotations into memory...\n",
      "Done (t=0.20s)\n",
      "Creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Base imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# Custom imports\n",
    "from model import InfusionNet\n",
    "import tools.dct as dct_tools\n",
    "# Yolo imports\n",
    "sys.path.append('./yolov7/')\n",
    "import yolov7.models.yolo as yolov7\n",
    "\n",
    "flir_path = '../data/FLIR/'\n",
    "llvip_path = '../data/LLVIP/'\n",
    "\n",
    "flir_rgbimages = os.listdir(flir_path + 'images_rgb_train/data')\n",
    "flir_thermalimages = os.listdir(flir_path + 'images_thermal_train/data')\n",
    "llvip_rgbimages = os.listdir(llvip_path + 'visible/train')\n",
    "\n",
    "llvip_thermalimages = os.listdir(llvip_path + 'infrared/train')\n",
    "\n",
    "print('Number of FLIR RGB images: ', len(flir_rgbimages))\n",
    "print('Number of FLIR Thermal images: ', len(flir_thermalimages))\n",
    "\n",
    "print('Number of LLVIP RGB images: ', len(llvip_rgbimages))\n",
    "print('Number of LLVIP Thermal images: ', len(llvip_thermalimages))\n",
    "\n",
    "llvip = COCO(llvip_path + 'LLVIP.json') # load the dataset\n",
    "llvip_ids = llvip.getImgIds()\n",
    "img_obj = llvip.loadImgs([llvip_ids[1]])\n",
    "anns_obj = llvip.loadAnns(llvip.getAnnIds(imgIds=[llvip_ids[1]]))\n",
    "\n",
    "\n",
    "rgb_img = Image.open(llvip_path + 'visible/train/' + img_obj[0]['file_name'])\n",
    "ir_img = Image.open(llvip_path + 'infrared/train/' + img_obj[0]['file_name'])\n",
    "\n",
    "def plot_bbox(img, anns):\n",
    "    # draw bounding boxes\n",
    "    for ann in anns:\n",
    "        x, y, w, h = ann['bbox']\n",
    "        plt.plot([x, x+w, x+w, x, x], [y, y, y+h, y+h, y], linewidth=2, color='r')\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(rgb_img)\n",
    "# draw bounding boxes\n",
    "plot_bbox(rgb_img, anns_obj)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(ir_img)\n",
    "# draw bounding boxes\n",
    "plot_bbox(ir_img, anns_obj)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "def plot_dct(input, masked_freq, output):\n",
    "    gs = gridspec.GridSpec(1, 3)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.subplot(gs[0,0])\n",
    "    input = input.squeeze(0).permute(1,2,0)\n",
    "    plt.imshow(input)\n",
    "    plt.subplot(gs[0,1])\n",
    "    masked_freq = masked_freq.squeeze(0).permute(1,2,0)\n",
    "    plt.imshow(masked_freq)\n",
    "    plt.subplot(gs[0,2])\n",
    "    output = output.squeeze(0).permute(1,2,0)\n",
    "    plt.imshow(output)\n",
    "\n",
    "rgb_tensor = transforms(rgb_img).unsqueeze(0)\n",
    "dct_tensor = dct_tools.dct_2d(rgb_tensor, norm='ortho')\n",
    "masked_tensor = dct_tools.mask_image(dct_tensor, 0.1)\n",
    "masked_rgb = dct_tools.idct_2d(masked_tensor, norm='ortho')\n",
    "\n",
    "plot_dct(rgb_tensor, masked_tensor, masked_rgb)\n",
    "plot_bbox(rgb_tensor, anns_obj)\n",
    "\n",
    "ir_tensor = transforms(ir_img).unsqueeze(0)\n",
    "\n",
    "dct_tensor = dct_tools.dct_2d(ir_tensor, norm='ortho')\n",
    "\n",
    "masked_tensor = dct_tools.mask_image(dct_tensor, 0.05)\n",
    "\n",
    "masked_ir = dct_tools.idct_2d(masked_tensor, norm='ortho')\n",
    "\n",
    "plot_dct(ir_tensor, masked_tensor, masked_ir)\n",
    "plot_bbox(ir_tensor, anns_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192, 256, 320])\n"
     ]
    }
   ],
   "source": [
    "from model import InfusionNet\n",
    "\n",
    "model = InfusionNet(num_features=64, reduction=32, tau = 0.2)\n",
    "\n",
    "input_tensors = torch.cat((torch.cat((rgb_tensor, ir_tensor), dim=1),torch.cat((rgb_tensor, ir_tensor), dim=1)), dim=0)\n",
    "\n",
    "output_tensor = model(input_tensors)\n",
    "\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yolov7_LLVIP.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myolov7(x)\n\u001b[1;32m     10\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 12\u001b[0m model \u001b[39m=\u001b[39m InfusionDetection(num_features\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, reduction\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, tau \u001b[39m=\u001b[39;49m \u001b[39m0.2\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m input_tensors \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((torch\u001b[39m.\u001b[39mcat((rgb_tensor, ir_tensor), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),torch\u001b[39m.\u001b[39mcat((rgb_tensor, ir_tensor), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m output_tensor \u001b[39m=\u001b[39m model(input_tensors)\n",
      "Cell \u001b[0;32mIn [6], line 5\u001b[0m, in \u001b[0;36mInfusionDetection.__init__\u001b[0;34m(self, num_features, reduction, tau)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39msuper\u001b[39m(InfusionDetection, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfusion \u001b[39m=\u001b[39m InfusionNet(num_features, reduction, tau)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myolov7 \u001b[39m=\u001b[39m yolov7\u001b[39m.\u001b[39;49mModel(\u001b[39m'\u001b[39;49m\u001b[39myolov7_LLVIP.yaml\u001b[39;49m\u001b[39m'\u001b[39;49m, ch\u001b[39m=\u001b[39;49m\u001b[39m192\u001b[39;49m, nc\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m/workspace/Infusion-Net_unofficial/src/yolov7/models/yolo.py:517\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, cfg, ch, nc, anchors)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39myaml\u001b[39;00m  \u001b[39m# for torch hub\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myaml_file \u001b[39m=\u001b[39m Path(cfg)\u001b[39m.\u001b[39mname\n\u001b[0;32m--> 517\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(cfg) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    518\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myaml \u001b[39m=\u001b[39m yaml\u001b[39m.\u001b[39mload(f, Loader\u001b[39m=\u001b[39myaml\u001b[39m.\u001b[39mSafeLoader)  \u001b[39m# model dict\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[39m# Define model\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov7_LLVIP.yaml'"
     ]
    }
   ],
   "source": [
    "class InfusionDetection(nn.Module):\n",
    "    def __init__(self, num_features=64, reduction=32, tau = 0.2):\n",
    "        super(InfusionDetection, self).__init__()\n",
    "        self.infusion = InfusionNet(num_features, reduction, tau)\n",
    "        self.yolov7 = yolov7.Model('yolov7_LLVIP.yaml', ch=192, nc=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.infusion(x)\n",
    "        x = self.yolov7(x)\n",
    "        return x\n",
    "\n",
    "model = InfusionDetection(num_features=64, reduction=32, tau = 0.2)\n",
    "\n",
    "input_tensors = torch.cat((torch.cat((rgb_tensor, ir_tensor), dim=1),torch.cat((rgb_tensor, ir_tensor), dim=1)), dim=0)\n",
    "\n",
    "output_tensor = model(input_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce05b5a9bede28b862a1c4f446dcff6069cc570518307c655629599948881620"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
